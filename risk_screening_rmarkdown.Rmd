---
title: "Screening Text for Risk Programmatically"
author: "Michael Mullarkey"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: no
      smooth_scroll: no
geometry: margin=0.50in
---

```{r setup, include=FALSE, cache = FALSE}
require("knitr")
## setting working directory
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, include = FALSE)

```

```{r loading packages}

if(!require(tidymodels)){install.packages('tidymodels')}
library(tidymodels)
if(!require(tidyverse)){install.packages('tidyverse')}
library(tidyverse)
if(!require(tidytext)){install.packages('tidytext')}
library(tidytext)
if(!require(readr)){install.packages('readr')}
library(readr)
if(!require(skimr)){install.packages('skimr')}
library(skimr)
if(!require(glue)){install.packages('glue')}
library(glue)
if(!require(diffdf)){install.packages('diffdf')}
library(diffdf)
if(!require(doMC)){install.packages('doMC')}
library(doMC)


```

```{r loading labeled tweet data to test whether it works}

## Data labeled for SITB or no SITB content already and described in this post: https://towardsdatascience.com/building-a-suicidal-tweet-classifier-using-nlp-ff6ccd77e971

urlfile="https://raw.githubusercontent.com/AminuIsrael/Predicting-Suicide-Ideation/master/suicidal_data.csv"

sitb_text <- read_csv(url(urlfile))

# No leading or trailing whitespace in the character variables

skim(sitb_text)

# Can also look at the data using glimpse

glimpse(sitb_text)

# Let's convert to more meaningful metric rather than 1/0

sitb_text <- sitb_text %>% 
  mutate(label = factor(case_when(
    label == 1 ~ "SITB",
    label == 0 ~ "No SITB",
    TRUE ~ NA_character_
    
  ))) %>% 
  na.omit() # Dropping NAs

# Let's see the breakdown of SITB/No SITB Tweets, looks pretty well-balanced

sitb_text %>% 
  count(label) %>% 
  mutate(percent = 100*(n/sum(n)))

```
```{r detecting presence of certain risk related words}

# What we want is first a tidy function that can return Yes or No for the presence of a certain word/regular expression, which we can do with stringr

# No function

no_func_ex <- sitb_text %>% 
  mutate(suic = str_detect(tweet, "suic")) %>% 
  print()

# Tidy function

detect_word <- function(.data, word){
  
  .data %>% 
  mutate("{word}" := str_detect(tweet, word))
  
}

tidy_ex <- sitb_text %>% 
  detect_word(word = "suic") %>% 
  print()

# Testing base and tidy function return the same dataframe

diffdf(no_func_ex, tidy_ex)

# Look at the percent of tweets that have an explicit mention of suicide in them by label, and see that there is mention of suicide in some tweets not labeled as containing SITB

tidy_ex %>% 
  group_by(label, suic) %>% 
  tally() %>% 
  mutate(percent = 100*(n/sum(n)))

## Now want to map over a bunch of words and attach them to the original dataframe

words <- c("suicid", "(?<!s)kill", "(?<!stu|la)die(?!go)", "death", "dead(?!line)", "harm", "hurt", 
    "murder", "cut(?!e)", "stab", "burn", "slice", "slash", "slit", "split", 
    "(?<!c)hang(?! *out|ing *out| *around|ing *around)", 
    "shoot", "fire(?!d)", "lynch", "blow.*up", "blow.*brains", "blow.*head", 
    "knife", "razor", "blade", "trigger", "gun", "rifle", "pistol", "glock", "revolver", 
    "muzzle", "bomb", "fertilizer", "rope", "noose", 
    "wrist", "throat", "vein", "artery", 
    "blood", "bleed", 
    "overdose", "pill", "needle", "poison", "bleach", 
    "(?<!anti)depress", "despise", "hate", "worthless", "burden", 
    "end.*life", "take.*life", "better *off", "who *cares", "why *bother", 
    "doesn'?t *matter", "does *not *matter", 
    "(?<!never |don't |don't ever |do not ever |dont |dont ever )give *up", 
    "gave *up", "given *up", 
    "no *one *underst", "nobody *underst",
    "no *one *get", "nobody *gets",
    "no *hope", "don'?t *have *hope", "not *have *hope", "lost *hope", 
    "crazy", "dark", "abus")

sitb_detected <- map_dfc(words, ~{
  
  sitb_text %>% 
    detect_word(word = .x)
  
}) %>% 
  dplyr::select(-contains("label"),-contains("tweet")) %>% 
  bind_cols(sitb_text) %>% 
  rowwise() %>% 
  mutate(any_detect = any(c_across(suicid:abus)),
         sum_detect = sum(c_across(suicid:abus), na.rm = T)) %>% # Seeing if we can detect any of the words
  ungroup() %>% 
  relocate(label, any_detect, sum_detect, tweet, everything()) %>% 
  arrange(desc(sum_detect)) %>% 
  print()

```

```{r what proportion of SITB and non SITB tweets have these words in them}

# See what percentage of the SITB and non=SITB labeled tweets have any word detected as true

sitb_detected %>% 
  group_by(label, any_detect) %>% 
  tally() %>% 
  mutate(percent = 100*(n/sum(n)))

sitb_detected %>% 
  mutate(any_detect = factor(case_when(
    
    any_detect == FALSE ~ "No SITB",
    any_detect == TRUE ~ "SITB",
    TRUE ~ NA_character_
    
  ))) %>% 
conf_mat(truth = label, estimate = any_detect) %>% 
  summary()


```
```{r looking at breakdown of number of risk words used in the tweet}

sitb_detected %>% 
  group_by(label, sum_detect) %>% 
  tally() %>% 
  mutate(percent = 100*(n/sum(n)))

```

```{r using just the presence absence and sum data to predict label}

# Removing the text

sitb_no_text <- sitb_detected %>% 
  dplyr::select(-tweet)

# Doing a reproducible training/testing split

set.seed(33)
split_sitb <- initial_split(sitb_no_text, prop = 0.8, strata = label)

train_sitb <- training(split_sitb)
test_sitb <- testing(split_sitb)

# Creating a boosted tree model

xgb_spec <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune(),                         ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

# We'll just use grid search here

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), train_sitb),
  learn_rate(),
  size = 30
)

# Create recipe

rec_boost <- 
  recipe(label ~ ., data = train_sitb)

## And add the formula

xgb_wf <- workflow() %>%
  add_recipe(rec_boost) %>%
  add_model(xgb_spec)

xgb_wf

# Let's create cv folds

set.seed(33)
sitb_folds <- vfold_cv(train_sitb, strata = label)

# Setting up multicore

doMC::registerDoMC(cores = 7)

# Tuning

xgb_res <- tune_grid(
  xgb_wf,
  resamples = sitb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

```


```{r running the final boosted tree model with best hyperparameters}

best_auc <- select_best(xgb_res, "roc_auc")

final_xgb <- xgb_wf %>% 
  finalize_workflow(best_auc)

tic()
set.seed(33)
boost_fit_rs <-
  final_xgb %>% 
  fit_resamples(train_sitb, control = keep_pred)
toc()

boost_fit_rs %>% 
  collect_metrics()


```



